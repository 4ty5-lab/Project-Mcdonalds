#!/usr/bin/env python3
"""
NLP File Organizer - Smart file organization with ML

"""

import os
import pickle
import re
import numpy as np
import spacy
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.preprocessing import LabelEncoder
from datetime import datetime, timedelta
import hashlib
from collections import defaultdict
import argparse  # TODO: Actually implement CLI args
import sys

# Shortcuts because I'm lazy
dt = datetime
dd = defaultdict
td = timedelta
# np already short enough lol

# Download spacy model if missing (this always breaks on new machines)
try:
    nlp = spacy.load("en_core_web_sm", disable=["parser", "ner"])
except OSError:
    print("Downloading spacy model... this might take forever")
    import subprocess
    subprocess.run([sys.executable, "-m", "spacy", "download", "en_core_web_sm"], check=True)
    nlp = spacy.load("en_core_web_sm", disable=["parser", "ner"])
    # NOTE: Sometimes this fails on corporate networks, need to handle that


class TextProcessor:
    """Cleans text for ML processing"""
    __slots__ = ['stop_words', 'punct_re', 'nlp']  # memory optimization
    
    def __init__(self):
        self.nlp = nlp
        self.stop_words = self.nlp.Defaults.stop_words
        self.punct_re = re.compile(r'[^\w\s]')  # kill all punctuation
        self.nlp.max_length = 1000000  # handle big files
        # TODO: make this configurable

    def clean_text(self, text):
        """Clean and lemmatize text"""
        if not text:
            return ""  # edge case handling
        
        text = text.lower()  # normalize case
        text = self.punct_re.sub(' ', text)  # remove punctuation
        
        # NLP magic happens here
        doc = self.nlp(text)
        # Filter out stop words and single chars
        return ' '.join([t.lemma_ for t in doc if t.lemma_ not in self.stop_words and len(t.lemma_) > 1])
        # FIXME: this might be slow for large texts


class FileOrganizer:
    """Main file organization class using ML"""
    __slots__ = ['model_dir', 'text_processor', 'vectorizer', 'classifier', 'label_encoder', 'category_stats']
    
    def __init__(self, model_dir="file_models"):
        self.model_dir = model_dir
        self.text_processor = TextProcessor()
        
        # ML components
        self.vectorizer = TfidfVectorizer(max_features=1500, ngram_range=(1,2))  # tried 2000 features, was too slow
        self.classifier = GradientBoostingClassifier(n_estimators=100, verbose=0)  # GB works better than RF here
        self.label_encoder = LabelEncoder()
        self.category_stats = dd(int)  # track category usage
        self._setup()

    def _setup(self):
        """Initialize directories and load saved models"""
        os.makedirs(self.model_dir, exist_ok=True)
        self._load_saved_models()

    def _load_saved_models(self):
        """Load previously trained models if they exist"""
        try:
            # Load vectorizer
            with open(os.path.join(self.model_dir, 'vec.pkl'), 'rb') as f:
                self.vectorizer = pickle.load(f)
            # Load classifier
            with open(os.path.join(self.model_dir, 'clf.pkl'), 'rb') as f:
                self.classifier = pickle.load(f)
            # Load label encoder
            with open(os.path.join(self.model_dir, 'le.pkl'), 'rb') as f:
                self.label_encoder = pickle.load(f)
            # Load stats
            with open(os.path.join(self.model_dir, 'stats.pkl'), 'rb') as f:
                self.category_stats = pickle.load(f)
        except (FileNotFoundError, EOFError):
            pass  # first run, no models exist yet

    def _save_models(self):
        """Save trained models to disk"""
        # TODO: add versioning to models
        with open(os.path.join(self.model_dir, 'vec.pkl'), 'wb') as f:
            pickle.dump(self.vectorizer, f)
        with open(os.path.join(self.model_dir, 'clf.pkl'), 'wb') as f:
            pickle.dump(self.classifier, f)
        with open(os.path.join(self.model_dir, 'le.pkl'), 'wb') as f:
            pickle.dump(self.label_encoder, f)
        with open(os.path.join(self.model_dir, 'stats.pkl'), 'wb') as f:
            pickle.dump(self.category_stats, f)

    def _get_file_info(self, file_path):
        """Extract features from file for classification"""
        try:
            name = os.path.basename(file_path)
            ext = os.path.splitext(name)[1].lstrip('.').lower()
            folder = os.path.basename(os.path.dirname(file_path))
            size = os.path.getsize(file_path)
            
            # Combine features into text
            features = [name, ext, folder, f"s{size//1024}"]  # size in KB
            
            # Read text files for content analysis (small files only)
            if ext in ['txt', 'md'] and size < 1e6:  # 1MB limit
                try:
                    with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
                        features.append(f.read(500))  # first 500 chars only
                except Exception:
                    pass  # file reading failed, skip content
                    
            return self.text_processor.clean_text(' '.join(features))
        except Exception:
            return ""  # something went wrong, return empty

    def train_categorizer(self, file_paths, categories):
        """Train the ML model on file examples"""
        if len(set(categories)) < 2:
            return 0.0  # need at least 2 categories
            
        # Extract features from files
        X_text = [self._get_file_info(fp) for fp in file_paths]
        X = self.vectorizer.fit_transform(X_text)  # convert to vectors
        y = self.label_encoder.fit_transform(categories)  # encode labels
        
        # Train classifier
        self.classifier.fit(X, y)
        score = self.classifier.score(X, y)  # training accuracy
        
        # Update stats
        for cat in categories:
            self.category_stats[cat] += 1
            
        self._save_models()
        return score

    def categorize_files(self, file_paths):
        """Predict categories for new files"""
        try:
            X_text = [self._get_file_info(fp) for fp in file_paths]
            X = self.vectorizer.transform(X_text)  # use existing vectorizer
            predictions = self.classifier.predict(X)
            return self.label_encoder.inverse_transform(predictions)
        except Exception:
            return [None] * len(file_paths)  # prediction failed

    def get_category_stats(self):
        """Return category usage statistics"""
        return dict(self.category_stats)


class WebManager:
    """Manages browser tabs and favorites with NLP search"""
    __slots__ = ['data_file', 'text_processor', 'tabs', 'favorites', 'tab_id_map']
    
    def __init__(self, data_file="web_data.pkl"):
        self.data_file = data_file
        self.text_processor = TextProcessor()
        self.tabs = {}  # active tabs
        self.favorites = {}  # bookmarks
        self.tab_id_map = {}  # tab_id -> url_hash mapping
        self._load_data()

    def _load_data(self):
        """Load saved web data"""
        if os.path.exists(self.data_file):
            try:
                with open(self.data_file, 'rb') as f:
                    data = pickle.load(f)
                    self.tabs = data.get('tabs', {})
                    self.favorites = data.get('favorites', {})
                    # Rebuild tab_id mapping
                    self.tab_id_map = {v['id']: k for k, v in self.tabs.items()}
            except (Exception, EOFError):
                # Corrupted file or first run
                self.tabs = {}
                self.favorites = {}
                self.tab_id_map = {}

    def _save_data(self):
        """Save web data to disk"""
        data = {'tabs': self.tabs, 'favorites': self.favorites}
        with open(self.data_file, 'wb') as f:
            pickle.dump(data, f)

    def add_tab(self, tab_id, url, title, content=""):
        """Add new browser tab"""
        url_hash = hashlib.sha1(url.encode()).hexdigest()  # unique ID for URL
        processed_title = self.text_processor.clean_text(title)
        
        self.tabs[url_hash] = {
            'id': tab_id,
            'url': url,
            'title': title,
            'clean_title': processed_title,  # for search
            'last_active': dt.now(),
            'visits': 1,
            'load_time': dt.now().timestamp()
        }
        self.tab_id_map[tab_id] = url_hash
        self._save_data()

    def update_activity(self, tab_id):
        """Update tab activity timestamp"""
        if tab_id in self.tab_id_map:
            url_hash = self.tab_id_map[tab_id]
            tab = self.tabs[url_hash]
            tab['last_active'] = dt.now()
            tab['visits'] += 1
            self._save_data()
            return True
        return False

    def find_inactive_tabs(self, threshold_min=15):
        """Find tabs that haven't been used recently"""
        threshold = td(minutes=threshold_min)
        now = dt.now()
        inactive = []
        
        for tab in self.tabs.values():
            if now - tab['last_active'] > threshold:
                minutes_inactive = (now - tab['last_active']).total_seconds() / 60
                inactive.append((tab['id'], minutes_inactive))
                
        return inactive

    def search_tabs(self, query, limit=10):
        """Search tabs using NLP similarity"""
        if not query or not self.tabs:
            return []
            
        clean_query = self.text_processor.clean_text(query)
        query_doc = nlp(clean_query)
        results = []
        
        for tab in self.tabs.values():
            # Calculate similarity between query and tab title
            sim = query_doc.similarity(nlp(tab['clean_title']))
            if sim > 0.3:  # similarity threshold
                results.append((tab['id'], tab['title'], tab['url'], sim))
                
        # Sort by similarity score
        return sorted(results, key=lambda x: x[3], reverse=True)[:limit]

    def add_favorite(self, url, title, category="general"):
        """Add URL to favorites"""
        url_hash = hashlib.sha1(url.encode()).hexdigest()
        now = dt.now()
        
        self.favorites[url_hash] = {
            'url': url,
            'title': title,
            'category': category,
            'added': now,
            'last_accessed': now,
            'access_count': 1
        }
        self._save_data()
        return True

    def remove_favorite(self, url):
        """Remove URL from favorites"""
        url_hash = hashlib.sha1(url.encode()).hexdigest()
        if url_hash in self.favorites:
            del self.favorites[url_hash]
            self._save_data()
            return True
        return False

    def get_favorites(self, category=None):
        """Get favorites, optionally filtered by category"""
        if category:
            return [fav for fav in self.favorites.values() if fav['category'] == category]
        return list(self.favorites.values())

    def search_favorites(self, query, category=None):
        """Search favorites using NLP"""
        if not query or not self.favorites:
            return []
            
        clean_query = self.text_processor.clean_text(query)
        query_doc = nlp(clean_query)
        results = []
        
        favorites = self.get_favorites(category)
        for fav in favorites:
            # Search in title, URL, and category
            text = fav['title'] + ' ' + fav['url'] + ' ' + fav['category']
            sim = query_doc.similarity(nlp(text))
            if sim > 0.2:  # lower threshold for favorites
                results.append((fav['title'], fav['url'], fav['category'], sim))
                
        return sorted(results, key=lambda x: x[3], reverse=True)

    def access_favorite(self, url):
        """Record favorite access"""
        url_hash = hashlib.sha1(url.encode()).hexdigest()
        if url_hash in self.favorites:
            fav = self.favorites[url_hash]
            fav['last_accessed'] = dt.now()
            fav['access_count'] += 1
            self._save_data()
            return fav
        return None


# CLI Commands (shortcuts for power users)
def cmd_organize(organizer, path):
    """Organize files in directory"""
    # TODO: implement this
    pass

def cmd_search_tabs(web_manager, query):
    """Search browser tabs"""
    results = web_manager.search_tabs(query)
    for tab_id, title, url, score in results:
        print(f"[{tab_id}] {title} - {url} (score: {score:.2f})")

def cmd_add_fav(web_manager, url, title, category="general"):
    """Add favorite bookmark"""
    web_manager.add_favorite(url, title, category)
    print(f"Added {title} to {category}")

def cmd_search_favs(web_manager, query, category=None):
    """Search favorites"""
    results = web_manager.search_favorites(query, category)
    for title, url, cat, score in results:
        print(f"{title} ({cat}) - {url} (score: {score:.2f})")

# Quick shortcuts
org = cmd_organize  # shortcut
st = cmd_search_tabs  # shortcut
af = cmd_add_fav  # shortcut
sf = cmd_search_favs  # shortcut


if __name__ == "__main__":
    # Initialize components
    organizer = FileOrganizer()
    web_manager = WebManager()
    
    # Demo data (remove this later)
    web_manager.add_tab(1, "https://python.org", "Python Programming Language")
    web_manager.add_tab(2, "https://github.com", "GitHub - Code Hosting")
    
    web_manager.add_favorite("https://python.org", "Python Official Site", "programming")
    web_manager.add_favorite("https://github.com", "GitHub", "development")
    web_manager.add_favorite("https://stackoverflow.com", "Stack Overflow", "programming")
    
    # Test search
    print("Search results for 'programming':")
    for result in web_manager.search_favorites("programming"):
        print(f"- {result[0]} ({result[1]})")
        
    print("\nFile organizer stats:", organizer.get_category_stats())
    
    # TODO: Add proper CLI interface
    # TODO: Add config file support
    # FIXME: Error handling needs work
    # NOTE: This is still a prototype
